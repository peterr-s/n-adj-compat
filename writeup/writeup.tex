\documentclass[a4paper, 11pt]{scrartcl}
%\documentclass[a4paper, 11pt]{article}
%\usepackage[margin=2.7cm]{geometry}     % sets the borders to 2.7cm each
\usepackage[english]{babel}             % defines language for spacing
\usepackage[utf8]{inputenc}             % allows entering special characters
\usepackage[T1]{fontenc}                % sets font to T1 and allows umlaute
\usepackage{lmodern}                    % improves font display in PDFs
\usepackage{microtype}                  % improves spacing when using lmodern

\usepackage[
	citestyle   = authoryear-comp,      % use compressed author-year citation
%	bibstyle    = JME,                  % use JME-style; change to JME_sentencecase to have all titles converted to lowercase letters
	maxbibnames = 5,                    % maximum number of names printed in bibliography before truncation with ``et al.'' is used
	minbibnames = 1,                    % number of authors displayed if truncation happens
	maxnames    = 4,                    % maximum number of names printed in citation before et al. is used
	minnames    = 1,                    % number of authors displayed if truncation happens
	datezeros   = false,                % no leading 0 if dates are printed
	date        = long,
	isbn        = false,                % show no ISBNs
	natbib      = true,                 % enable natbib-compatibility
	url         = false,                % show no urls
	backend     = bibtex                % use bibtex as backend
	]{biblatex}

\addglobalbib{bibliography}             % defines the name of the .bib file

\setkomafont{disposition}{\normalfont}

\begin{document}

\title{Semantic Compatibility of Nouns and Adjectives on the Basis of Word Embeddings}
\author{Peter Schoener}
\date{\today}
\maketitle
\clearpage

\tableofcontents
\clearpage

\section{Introduction}

In response to growing interest in applying distributional approaches to problems that require consideration of entire phrases rather than just words, and in order to predict more accurate embeddings than would be found distributionally for compound words, several methods have been developped to compose multiple embeddings. Parsing is immediately related to composition, since the items being composed would be those which together make up a phrase.

Of course, when training compositional models, the goal is to predict an embedding. This must be done using positive samples, since it is impossible to generate gold standard negative targets other than noise, which would clearly harm the ability to predict valid compositions. Not only does this mean the composition models are unable to predict invalid compositions, these would likely not be useful in the first place. The more advanced models are not associative or commutative, so even as part of a larger phrase a prediction for an invalid pair would make the final output less, if at all, valid. Indeed, a meaningless composition is one of the better possible outcomes in this case \textemdash the embedding might also collide with an unrelated word or composition, causing misleading results.

It is therefore important to have an accurate parse so that the resulting composition is useful, or to filter after parsing but before composition and flag pairs that can not be composed. By checking possible transitions or terminals when parsing, it would be possible to get a higher parse accuracy, which, while immensely helpful for composition as explained above, is useful in any other domain requiring accurate parsing as well.

Existing work on compatibility centers mainly on mutual information and related statistical metrics, which are only applicable to known words. Embeddings, on the other hand, do not require the word to have been seen, or at least not as often. With subword composition methods such as FastText (Mikolov et al.), embeddings can be generated for words not seen at all, and those seen rarely will be trained if similar words have been seen sufficiently often.

The goal of this project is therefore to create a model that can determine whether a given embedding pair is semantically compatible. Because it is a first attempt at such an approach, the scope is limited, but given the results the models can hopefully be generalized to arbitrary embedding pairs.

Moving one step further, if composition functions are to be applied multiple times, the compatibility checking will need to work either with the entire phrases, which is untenable for even slightly long sequences, or work on the output of the previous compositions. While composed embeddings are beyond the scope of this project, they would be a logical extension of it.

Because of the ease of extracting noun-adjective pairs and because they tend to be very clearly valid or invalid, the first tests were done on these. However, the approaches that worked for them were later tested on and expanded to verb-direct object pairs.

We tested several different models of increasing complexity, the goal being to find the simplest architecture that can achieve high accuracy. First, logistic regression and a feed-forward neural network predicting mutual information, with a cutoff trained to predict validity, then the same but without predicting mutual information. Lastly, we adapted two proven composition models, FullLex and transformation selection, to binary classification and tested against them.

Before presenting the actual results we investigate existing work on compatibility and the cutting edge of composition, from which a few reasoned approaches emerge. These can then be tested against each other and against existing methods, and tentatively broadened to more varied word and embedding types.


\section{Related Work}

\subsection{Statistical approaches to compatibility}

Existing work on compatibility has been primarily on the basis of statistical metrics applied to concrete words. Conditional probability and cooccurrence have been shown to be effective (Martin Volk), and the lexical association metric (Hindle and Rooth) also works. Pointwise mutual information can likewise be adapted to a highly effective determiner of compatibility (van Noord).

The earliest of these works was that by Hindle and Rooth, whose approach is elegant and quick, but has a few pitfalls. Firstly, it is only a comparison metric that can disambiguate between two possibilities, rather than determining absolute compatibility. Secondly, it requires that the noun-preposition attachment have been seen, as have both candidates in isolation, else the metric becomes undefined. Thirdly, it requires that the verb attachment and the unattached noun have been seen or else the metric assigns a value of complete uncertainty. Lastly, even when these have all been seen, performance remains low when they have not been seen very often.

The cooccurrence metric (Volk) has less trouble deciding between the attachment candidates, but is still ultimately a comparison rather than an absolute metric. However, it achieves great accuracy while retaining the speed benefits of a statistical approach. With over 90\% decidability and 80\% accuracy on those decidable cases, it sets as a precedent an accuracy that might be considered a meaningful target to maintain for further work, which could expand to cases beyond the ambiguous ones used to more general attachments and unseen tokens.

Another important conclusion, arguably more central to the point of the same paper, is that unsupervised approaches perform equally well as unsupervised ones for this sort of task. This opens up the possibility of using massive amounts of automatically annotated data rather than a limited manually annotated set. More concretely, this suggests that the type of data needed to effectively train good embeddings and generalizeable transformations for said embeddings is indeed usable for this application.

Similar methods have also been adapted to absolute metrics (van Noord). This $$ I(r(w_1, w_2)) = log(\frac{f(r(w_1, w_2))}{f(r(w_1, \_ )) \cdot f(\_ (\_ , w_2))}) $$ becomes very similar to the PMI $$ log(\frac{f(r(x, y))}{f(r(x, \_ )) \cdot f(r(\_ , y))}) $$ of the two words for which the compatibility is to be determined. While this metric was not tested on its ability to predict whether or not a pair is at all compatible, it does perform very well for disambiguating verb-subject-object attachments.

Note that this makes it, as tested so far, also a relative metric. However, it should ideally assign a higher score to any compatible pair than to any incompatible pair, at least for a given verb around which the disambiguation is to be done. This means that at the very least it should be possible to find a threshold above which a noun can be considered compatible with a given verb. Whether this threshold is consistent across verbs is not important to the paper and therefore not investigated, but would be equivalent to the metric being applicable as a classifier of compatibility or incompatibility.

While not using the exact same measures, there has been research into using similar statistical methods to determine compatibility of a single candidate in a given type of relation, and indeed relatively simple measures of cooccurrence seem to work quite well for this task (Yang, Su, Tan, 2005). However, the particular domain in question was English pronoun attachment, which is not as broad as the other types of compatibility discussed above. Nonetheless the good performance of the model does not rule out that finding a single cutoff for cooccurrence across the task might be a sensible approach.

The exact measure used here is not as directly related to mutual information as those used by, for example, van Noord, incorporating the rank of a candidate within the space of cooccurrences between the given and all competing candidates. Note that this is still not the same thing as training separate models for each element of one of the categories of words: the rank is used merely as a factor, weighting a measure similar to pointwise mutual information. Thus, while it draws on a slightly more complex measure, this may suggest that pointwise mutual information or a similar measure could be used to set a single cutoff.

Given that this is the case, a metric which ranks pairs by co-occurence should be applicable to this task, possibly allowing a simple threshold to be set in order to discriminate between valid and invalid pairs. The question remains, however, which metric should be used. Normalized PMI has a few useful properties for tasks of this nature (Bouma), most importantly that it counteracts PMI's bias toward rare collocations and, as a direct result of its nature, has a fixed interpretation; once the cutoff is determined the space of accepted tokens can be expanded without completely retraining the model. Of course, since this project is designed to work on word embeddings, this would mean first training a predictor for NPMI and then finding the appropriate cutoff.

\subsection{Embedding composition}

As far as embedding-based approaches, the only existing work is in composition itself. Two notably successful architectures for compostion are FullLex (Socher et al. 2012) and transformation selection (de Kok, Dima). Both of these involve applying a transformation to each constituent and then another to the concatenation of the outputs, the difference being in whether one or all of the possible transformations is applied.

Assuming one can accurately predict what the composition should be if the constituents are compatible, it stands to reason that a valid angle might be checking the resultant embedding for validity. The ideal method for the latter operation is less well-explored. However, since the goal here is to take a neural approach, it may be possible to train adaptations of the aforementioned composition models to simply output a binary classification directly or with one extra layer.

Embeddings can be trained to represent different properties of words. While it is clear that with a large corpus a high dimensionality and cutoff, e.g. the common values of 300 and 50, respectively, will lead to more accurate embeddings, this does not mean the represented information is the information we are looking for. Although our experiment uses Word2Vec embeddings, the paper introducing GloVe (Pennington, Socher, Manning) suggests that smaller windows will favor syntactic information, whereas larger windows favor semantic information.


\section{Preprocessing}

\subsection{Corpus}

The corpus used in this experiment is made up of articles published from 1986 to 2009 in the German Tageszeitung (taz) newspaper. It contains 28.9 million sentences and a total of 393.7 million tokens, tagged and parsed automatically. This is done with the Citar tagger, the parser presented in (de Kok, 2016), and the Lemming lemmatizer.

The tagger is about 96\% accurate (Plank, van Noord, 2011), the parser achieves just over 90\% LAS, and the lemmatizer is about 98\% accurate. It is important that the annotations be not only valid but complete so as to ensure that (1) the embeddings on which the model trains are indeed those of adjectives and nouns, (2) as many positive edge cases as possible are accurately modelled (a) so as to make the most meaningful impact on parsing and (b) so they are not caught by the negative sampling, actively degrading the accuracy of the model.

In terms of the data relevant to our experiment, there are 26.6 million noun-adjective pairs found in the corpus for which an embedding can be trained for at least one constituent with the 50 occurrence minimum. About 24.6 million of these are actually used in training, that is, both embeddings are present. The number of usable positive samples is the limiting factor in the total quantity of data, as we will see later.

\subsection{Embeddings}

The inputs to the neural network are Word2Vec embeddings trained over the entire dataset. A window size of 10 ensures that the embeddings carry more semantic than syntactic information, and a minimum count of 50 ensures that all the embeddings used have been trained well enough to be reasonably accurate representations. We settled on an embedding dimensionality of 300 to ensure that the network has as fine-grained information to work with as possible.

The ConLL data is searched for dependencies between nouns of any type and adjectives of any type. Any pairs for which an embedding is not found for either component are discarded since they will be of no use for training. Because any pair found in the corpus can be assumed to be valid, the positive samples on which the model is trained are simply all pairs which occur in the corpus on which it can be trained, i.e. all those for which embeddings could be generated.

The way positive samples are generated means that the minimum count for the embedding model has a further consequence beyond ensuring validity of the embeddings: the model will not attempt to learn based on rare words which might have strange or simply incompletely represented compatibility properties. Because the list is not deduplicated, the model learns primarily from pairs where the compatiblity is well-established.

\subsection{Negative sampling}

Of course, it does not make sense to train only on positive samples. Negative samples are generated first by random sampling, but then filtered against not only pairs that did occur, but those which are embedding-wise similar. In order to avoid doing a quadratic number of searches depending on the number of similar embeddings taken into account, we make a quadratic number of entries in a Bloom filter, which is of course much faster while still being memory efficient. The filter is designed to have a false positive rate of at most 5\% , though in reality this will be even lower since the element count is estimated directly from the number of positive samples and the filter does not need additional space for duplicate elements; it is substantially larger and sparser than it needs to be. The process is further sped up by keeping the neighbor lists in a hash map once they have been looked up, which costs memory at preprocessing time, but this step need be done only once and can be used with all the architectures examined in this thesis.

Each negative sample is generated and then tested against the Bloom filter, then scrapped and regenerated if it is found. This allows precise control over the number of negative samples. In order to maintain a balanced training set, I fixed this to the number of positive samples.

\subsection{Test data}

Test data is assembled from a few different sources. Some samples are invented, and the rest are drawn from human judgements on random elements from the positive and negative sets, as well as randomly generated samples. Unlike the negative training data, these random samples are not filtered against anything. By relying on human judgement misclassifications can be avoided, i.e. outliers that should not be in the positive set and valid random pairs that should not be in the negative set. Just as importantly, tokens incorrectly tagged as nouns or adjectives can be filtered out; errors when classifying these should not be held against the model since they are not valid inputs in the scope of this task.


\section{Models}

\subsection{Logistic Regression}
In order to determine whether it even makes sense to spend time training a slower, more complex, and less transparent architecture, I first tried applying logistic regression to this task. However, even with transformations to dimensions in the thousands, accuracy did not improve far beyond a guess, and higher-dimensional matrices provided no marginal benefit. For this reason I abandoned this approach and moved to architectures that allowed more complex interactions.

\subsection{Feed-Forward Neural Network}
The final version of this model is a feed-forward neural network with three hidden layers of 2000, 2000, and 1000 neurons. More neurons per layer did not yield any significant improvement, nor did more layers. When compared to a single hidden layer with 2000 neurons, this expansion gives an improvement in validation accuracy of about two percentage points. While not a huge leap, this is substantial enough to warrant the expansion. Moreover, by expanding until performance plateaus, it is possible to determine the limit of this approach.

The hidden layers use ReLU activation simply because it has been shown in previous experiments to work well and indeed gave better results than the hyperbolic tangent we had been using initially. The hidden layers have dropout with 80\% retention to prevent overfit, though with such a large dataset this is not a huge risk to begin with. Using less dropout does not improve accuracy either, so it certainly does not harm the model.

The output layer uses an approximation of the logistic (sigmoid) function to end up with a value between zero and one, which can be interpreted as a probability. In order to avoid continued adjustment of good weights, which would lead to them growing out of control, the output of the sigmoid is rounded if the input is beyond a certain cutoff. A good value for the cutoff turned out to be $ \pm 8 $.

Training is done using an Adam optimizer with a learning rate of 0.0025 in batches of 10000 samples. As it turns out, a learning rate over 0.005 is too high for the model to converge at all, and performance starts to pick up around 0.003. In order to maintain balanced batches, the entire test set is shuffled at the beginning of each epoch. The inputs to the model are the concatenations of the noun and adjective embeddings, i.e. a rank one tensor with 600 elements.

The training outputs are simply booleans \textemdash one for positive and zero for negative samples. An output of over 0.5 is considered a prediction that the pair is valid, and an output of 0.5 or lower the converse. Because an output of 0.5 means that the model is entirely unsure of how to categorize the pair, and because there are a roughly equal number of false positives and false negatives ignoring this case, we decided it would be more expedient in practical application to just guess than to create a third, uncertain category.

\subsection{FullLex adaptation}
Because of the success of the FullLex (citation) architecture in composition, it was reasonable to expect that transforming the noun embedding according to a matrix corresponding to the specific adjective and vice versa, with a final transformation on the concatenation of the two resulatant tensors, i.e. $$ W_o \times [W_v \times u; W_u \times v] + b $$ would yield even better results than the feed-forward approach. For simplicity and to ensure sufficient data was passed in, the embeddings of each category were clustered and each cluster mapped to a transformation matrix. The memory cost of training a separate matrix for each word in the known vocabulary was also untenable, with the raw data (not including overhead) on the order of a terabyte.

This approach uses the same batch size and learning rate as the feed-forward model, but was also tried with more dropout, as this is an important component of the similar transformation selection model. However, even retention as low as 10\% did not help the transformation matrices generalize. We train 100 such matrices for each the nouns and the adjectives, as this has been shown to be more than sufficient differentiation for this type of approach. Using fewer matrices and more data points per matrix does not improve performance.

This architecture takes considerably longer to train, roughly ten times as long on the same hardware as the feed-forward model. Some of this is due to the cost of looking up each input's cluster, since this can not be done in an efficient way without dereferencing the tensor, which is not possible due to the way Tensorflow sessions are managed in hybrid Rust/Python projects. This may actually be a case where a performance gain could be achieved by switching entirely to Python due to the incompleteness of the Tensorflow Rust API and the cost of the only simple way to do this operation.

One could also do the clustering in Rust, but in order to stay within the constraints of this thesis and not break compatibility with the other models, this will be left as a possible future development. However, it would not be a useful improvement, as shown later in this thesis.

\subsection{Transformation selection}
A similar approach to FullLex, transformation selection (de Kok, Dima), has been proposed. Rather than applying only the words' transformation matrices to each other, a large number of general matrices in the form of a rank 3 tensor are applied to each constituent and whichever of these happen to interact strongly with the embeddings then also have the largest impact on the output, and a nonlinearity, in our case the ReLU, is thrown into the mix, that is, $$ y_{pred} = W_o \times ReLU([W_U \times u + b_U; W_V \times v + b_U]) + b_o $$ This has proven even more effective at composition than FullLex. For this reason, an adaptation of transformation selection might be expected to outperform the adaptation of FullLex. If more accurate compositions can be predicted with this method, and still assuming that this implies it can model the arguably simpler interaction of whether they combine in the first place.

For this experiment, the hyperparameters follow the optima outlined in the paper introducing the method. The transformation tensors are each 80x300x300 (separate for nouns and adjectives), and dropout with only 10\% retention is used. This was applied first to random elements but then to entire matrices within the transformation tensors, but performance was the same either way. The provided model is still set up to use dropout on entire matrices.

Because of the rank of the transformation output and the need to reduce it to a single number, two outer transformations are applied. This has the added effect of essentially generating an embedding and then applying logistic regression to the output, i.e. applying logistic regression to determine the validity of a generated embedding.


\section{Results}

As mentioned earlier, the backpropagation uses log loss because of how well it works with percentages. Of course, the percentages are just confidences and not the final output per se. Because the task is binary classification, it makes sense to use the accuracy as a metric of performance, keeping an eye on the ratio of false positives to false negatives. The specific cases with which it struggles can also provide insight, of course, but with 300-dimensional embeddings and the nature of neural nets the reasons for misclassification will only be conjecture.

Furthermore, because all the training and validation data is automatically annotated, there are some errors that propagate into the classifier. The rate of things that are incorrectly tagged as nouns or adjectives is higher in the misclassified output than in the input, which bodes well for testing on gold standard data. Because one application for this project is as a step in making parse decisions, there is a bit of a feedback loop here. In order to make sure that it will work correctly and has not been trained too much on the errors, the model also needs to be evaluated on a hand-written list of noun-adjective pairs, both valid and invalid.

The most directly relevant and important measure is, of course, accuracy. However, depending on the task at hand, a low rate of false positives or false negatives might be more desirable, so these are provided as well, though one should note that for the sake of this thesis the models were made to err on the side of caution; absolutely uncertain cases are marked as incompatible. Rather than implement a cutoff for confidence to make a decision at all, we simply document the confidence with all the misclassified samples. Accuracy and error types are broken down in Table \ref{accuracy-an}.

\begin{table}[]
	\centering
	\begin{tabular}{l|lll|lll}
		                         & \multicolumn{3}{c|}{validation} & \multicolumn{3}{c}{test} \\
		                         & acc      & fp       & fn        & acc     & fp     & fn    \\ \hline
		logistic regression      & 62.1\%   & 16.5\%   & 21.5\%    &         &        &       \\
		feed-forward neural net  & 92       &          &           &         &        &       \\
		FullLex                  & 64.4     & 15.7     & 19.9      &         &        &       \\
		transformation selection & 60.5     & 22.2     & 17.3      &         &        &      
	\end{tabular}
	\caption{accuracy and error types on validation and test data}
	\label{accuracy-an}
\end{table}

It is immediately apparent that the feed-forward neural network is far and away the most accurate approach.

One interesting difference between the transformation selection and the other approaches is that it has a substantially higher rate of false positives than false negatives, despite ties going to the negatives as with the other models. This is hard to explain, but may be an effect of the final layer's function: because it is designed to implicitly train an embedding generator and then evaluate the output for validity, it could be that the final layer, simply due to the nature of how embeddings really are distributed within the space, tends to believe they are valid. If one considers the way that embeddings are trained, it makes sense that an embedding model will make relatively full use of the space available. This means that there will likely be few pockets that are semantically invalid areas, and this could extend to generated embeddings as well. Thus, the best guess would often be in favor of the embedding being valid, but in our case half of them are not.

One should also consider how severe the misclassifications are, and, being used as the loss function during training for exactly this reason, the [truncated] logarithmic loss is a good indicator of this. Of course, it still counts correct predictions which are insufficiently confident, but the nature of this function is that it penalizes severely wrong predictions far more heavily. The average loss for each approach is shown in Table \ref{loss-an}.

This is especially important in the case of test loss; given the high accuracy on test data, the loss may shed some light on which classifiers would be more reliably accurate on a broader test set; one would imagine that a model getting high scores with great confidence would generalize better than one doing the same with low confidence. However, such a result could equally mean that the model is trained very well for the sort of data in the test set particularly, but not for a more general domain.

\begin{table}[]
	\centering
	\begin{tabular}{l|l|l}
		                         & val. loss & test loss \\ \hline
		logistic regression      & 0.6352    &           \\
		feed-forward neural net  &           &           \\
		FullLex                  & 0.6214    &           \\
		transformation selection & 0.6416    &          
	\end{tabular}
	\caption{truncated (for inputs outside of $[-8, 8]$) logarithmic loss}
	\label{loss-an}
\end{table}

The results here are in line with what we saw in the accuracy rankings, with the values fairly clearly linked to the number of misclassified samples. This does not mean, however, that this data gives no additional information: It shows that none of the four models misclassifies with vastly more confidence than the others, or at least not on average.

Given the promising results, it made sense to expand the approaches to other types of embedding pairs. Since this is meant as a test of generalizability of the proposed models, we did not tune the hyperparameters separately for the new task. This being a secondary task, no human-annotated test set was prepared; the values shown are for validation data. The accuracy of the same architectures, retrained, is shown in Table \ref{accuracy-vn}.

\begin{table}[]
	\centering
	\begin{tabular}{l|lll}
		                         & acc      & fp       & fn        \\ \hline
		logistic regression      &          &          &           \\
		feed-forward neural net  &          &          &           \\
		FullLex                  &          &          &           \\
		transformation selection &          &          &
	\end{tabular}
	\caption{accuracy and error types on validation and test data}
	\label{accuracy-vn}
\end{table}

As before, the log loss can be a source of insight into the model; it is provided in Table \ref{loss-vn}.

\begin{table}[]
	\centering
	\begin{tabular}{l|l|l}
		                         & loss      \\ \hline
		logistic regression      &           \\
		feed-forward neural net  &           \\
		FullLex                  &           \\
		transformation selection &
	\end{tabular}
	\caption{truncated (for inputs outside of $[-8, 8]$) logarithmic loss}
	\label{loss-vn}
\end{table}

Another potentially interesting test was applying the verb-noun model to the noun-adjective task. While this would clearly not be as accurate as using the noun-adjective-specific model, it sheds some light on the feasibility of training a single more general model. This test is done on the human-annotated noun-adjective data because it is entirely valid, but also on the noun-adjective training data since it is more in line with what the model was trained on. These results are shown in Tables \ref{accuracy-vn-na} and \ref{loss-vn-na}.

\begin{table}[]
	\centering
	\begin{tabular}{l|lll}
		                         & acc      & fp       & fn        \\ \hline
		logistic regression      &          &          &           \\
		feed-forward neural net  &          &          &           \\
		FullLex                  &          &          &           \\
		transformation selection &          &          &
	\end{tabular}
	\caption{accuracy and error types on validation and test data}
	\label{accuracy-vn-na}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{l|l|l}
		                         & loss      \\ \hline
		logistic regression      &           \\
		feed-forward neural net  &           \\
		FullLex                  &           \\
		transformation selection &
	\end{tabular}
	\caption{truncated (for inputs outside of $[-8, 8]$) logarithmic loss}
	\label{loss-vn-na}
\end{table}


\section{Evaluation}

\subsection{Data}
The training data used in this thesis was quite accurately tagged and therefore the results are for the most part transferrable to practical applications, but it is not perfect. As mentioned before, as parsing improves, potentially even through application of compatibility checking techniques such as those presented here, so will the applicability of the model when trained on such data. While this thesis does not investigate the bounding effect of corpus quality on accuracy, this is an important factor to look at when trying to improve the model; it could be that the model is only being held back by the corpus, but it could equally be that the little noise present is already within the confines of what it can work around entirely.

Even with a perfectly annotated text corpus, the negative sample extraction is not perfect. Firstly, it relies on having a large anount of data, which essentially rules out human annotation, and secondly it allows, even with a large corpus, valid pairs to be added to the invalid set. Not all embedding-wise similar tokens will share the same compatibility characteristics as a given token, meaning some invalid pairs are left indeterminate as far as the training data is concerned. Conversely, not all those which do share the same characteristics will rank among the ten lowest cosines.

One reaction here might be to simply move to a cosine-cutoff-based approach, but synonym extraction has proven far more complex than this in practice. While this is not, strictly speaking, a synonym extraction problem, it is somewhat similar in that other parts of speech should be ruled out, dense areas need to be contended with, connotations play a role in context, etc. Clearly, a better negative sampling system would require further thought, but it would likely bring improvements, at the very least making the performance in the real world easier to guarantee.

The human-annotated data is in part generated by human judgements of randomly generated pairs. This means that the distribution of even the valid pairs is not in line with that which would be found in real-life text. While the specific incidences of misclassification can be taken as indicators of how well the model works, the accuracy rate does not necessarily reflect the accuracy in real world use. A common but misclassified pair is much worse than a rare misclassified one, and this is not reflected by the test set. This was part of the motivation behind using human judgements of the found pairs and those generated by negative sampling, but this is also not without its issues. While not the same as testing on training data \textemdash the pairs may switch categories \textemdash it may carry the same issues to an extent, since it does not help test whether the model has learned to generalize to pairs never encountered. This is arguably one of the more important categories, since certifying that a pair has been seen can be done with much simpler methods.

\subsection{Models}
One of the models presented in this thesis, namely the feed-forward neural network, performed quite well, but there are still a few things to consider when applying these results in practice, and room for improvement in this and the other methods. As mentioned above, it is difficult to determine whether this will transfer to other datasets with different but still real pair distributions; the test data offers a flatter but ultimately similar distribution. Also, training neural models takes a very long time, especially when separate models are trained for each type of pair. On a modern server with an nVidia Tesla M60, training to peak performance took about a day. For this reason, and because it would make it more transparent what properties of embeddings are indicators of compatibility, improvements in regression models would be worth pursuing.

The composition-based approaches are also worth investigating further; they have been shown to work well on composition and yet performed considerably worse on this related task. This could be a matter of vastly (since small tweaks gave no improvement) incorrect hyperparameter tuning, or of needing more layers after the composition-based output. The original attempt to model PMI with a feed-forward network and set a cutoff on it with a final layer was abandoned in order to free up the intermediate layers, but maybe with the composition-based approaches this sort of training could be made to work, i.e. essentially attaching a neural validator to the end of an existing composition model.


\section{Conclusion}
The goal of this thesis was to put forth a proof-of-concept model that used a neural approach to determine semantic compatibility. Indeed, it is possible to predict, with a high degree of accuracy, the compatibility of nouns with adjectives.

\section{Bibliography}
%\phantomsection                         % allows for correct link to Table of Contents
\addcontentsline{toc}{section}{References} % Adds the line "References" to Table of contents
%\onehalfspacing


\printbibliography                      % print the bibliography using BibLaTex

\section{Appendix}

\subsection{Data}

\subsection{Code}


\end{document}
